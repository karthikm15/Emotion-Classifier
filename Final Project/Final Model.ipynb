{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=font-size:40px> Personality Type Classification </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook is not a comprehensive list of the steps taken throughout this project but rather a summary of our final results.\n",
    "\n",
    "<h1> Implementation </h1>\n",
    "\n",
    "We mainly extracted images from eight different categories (anger, surprise, disgust, fear, neutral, happiness, sadness, and contempt) from a dataset found online: https://github.com/muxspace/facial_expressions\n",
    "\n",
    "Since some categories (like neutral and happiness) had more images than others, we synthesized new images by changing factors like the contrast, color, and sharpness and added it to these categories with fewer images. This allowed for an equal amount of images for each separate category. \n",
    "\n",
    "We subsequently split the data into training and validation datasets using a 60-40 split; we didn't leave images for testing as the online dataset had a separate section for testing images. At the end, there were ~4,000 images in each category for the training set and ~2,730 images in each category for the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing these images, we resized them from 350x350 to 50x50 to allow for faster model training time. Due to time and budget constraints, these images were resized, which could have led to a lower accuracy for the overall model; for future implementation, the images will be resized back to their original format in order to ensure maximum accuracy.\n",
    "\n",
    "In order to receive the make the most accurate predictive model, we implemented three approaches: using pre-existing Sagemaker image classification algorithms to produce the model, utilizing AWS Rekognition to predict the personality of the testing images, and building a convolutional neural network from scratch using Tensorflow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using Sagemaker Training Jobs </h2>\n",
    "\n",
    "To give Sagemaker access to the contents of these files, we shifted the training and validation datasets to S3 for better operability. We used training jobs which used the pre-built image classification algorithm to create a model based on the images sent to them (350x350) using a ml.p2.xlarge instance. The hyperparameters and inputs were changed accordingly in order to provide the most optimal model. Some notable changes that we made was to the number of epochs (15), the method of gradient descent (stochastic gradient descent), and the number of training samples (53336). The training time for this model averaged ~5 hours. \n",
    "\n",
    "The finished model was uploaded to S3 and given testing images in order to validate its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Testing the Training Jobs </h3>\n",
    "\n",
    "The Sagemaker training job had an overall training accuracy of ~88% and a validation training accuracy of ~81%. In order to test the training job on the testing images given by the dataset, the model was recompiled into a Jupyter notebook for further testing.\n",
    "\n",
    "Using the MXNet library, we extracted the model from the Sagemaker training job and used it to make a prediction. We selected a random test file and have shown the process of evaluating the model below for proof of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "training_job_name = 'image-classification-personality-training-job-try-7'\n",
    "job_info = boto3.client('sagemaker').describe_training_job(TrainingJobName=training_job_name)\n",
    " \n",
    "mx_model = mx.module.Module.load(\"../image-classification\", 0, False, label_names=['out_label'])\n",
    "\n",
    "def prepare(filepath, IMG_SIZE):\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE) \n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) \n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "def make_prediction_dense(model: mx.module, x_array: np.ndarray, batch_size: int=1):\n",
    "    data_iter = mx.io.NDArrayIter(data=x_array, batch_size=batch_size)\n",
    "    model.bind(data_shapes=data_iter.provide_data)\n",
    "    prediction = model.predict(data_iter).asnumpy().flatten()\n",
    "    return model, prediction\n",
    "\n",
    "unique_emotions = ['anger', 'surprise', 'disgust', 'fear', 'neutral', 'happiness', 'sadness', 'contempt']\n",
    "test_file = os.listdir(\"../test_resize\")[1]\n",
    "data = prepare(\"../test_resize/\" + test_file, 350).reshape(1, 1, 350, 350)\n",
    "model, prediction = make_prediction_dense(mx_model, data)\n",
    "prediction_list = prediction.tolist()\n",
    "max_value = max(prediction_list)\n",
    "maximum = -1\n",
    "for index in prediction_list:\n",
    "    if (maximum < index):\n",
    "        maximum = index\n",
    "unique_emotion = unique_emotions[prediction_list.index(maximum)]\n",
    "\n",
    "print(unique_emotion)\n",
    "\n",
    "plt.imshow(cv2.imread(\"../test_resize/\" + test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using AWS Rekognition </h2>\n",
    "\n",
    "Using the boto3 library, Rekognition was able to be used to train and test the model. We mainly focused on the 'emotions' section of Rekognition as this had the necessary information for predicting the personalities of the training images. Since the model was already pre-trained, there was no necessary configuration in order to train this model; therefore, we immediately began testing the model's accuracy. Rekognition often showed multiple emotions, so we simply took the emotion that had the highest probability according to Rekognition.\n",
    "\n",
    "This was done by using the images that were placed inside the S3 bucket (from the Sagemaker training jobs) and checking whether the labels of these images matched the labels that were seen by AWS Rekognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "rekognition = boto3.client('rekognition')\n",
    "\n",
    "# Accuracy of the AWS Model (using Rekognition): approximately 60 percent\n",
    "\n",
    "BUCKET = \"image-classification-rekognition\"\n",
    "KEY = 'sadness/Alain_Cervantes_0001.jpg'\n",
    "FEATURES_BLACKLIST = (\"Landmarks\", \"Emotions\", \"Pose\", \"Quality\", \"BoundingBox\", \"Confidence\")\n",
    "\n",
    "unique_emotions = ['anger', 'surprise', 'disgust', 'fear', 'neutral', 'happiness', 'sadness', 'contempt']\n",
    "related_emotion = ['ANGRY', 'SURPRISED', 'DISGUSTED', 'FEAR', 'CALM', 'HAPPY', 'SAD', 'CONFUSED']\n",
    "\n",
    "KEYS = []\n",
    "s3_client = boto3.client('s3')\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "result = paginator.paginate(Bucket='image-classification-rekognition',StartAfter='2018')\n",
    "for page in result:\n",
    "    if \"Contents\" in page:\n",
    "        for key in page[ \"Contents\" ]:\n",
    "            keyString = key[ \"Key\" ]\n",
    "            KEYS.append(keyString)\n",
    "\n",
    "def detect_faces(bucket, key, attributes=['ALL'], region=\"us-east-1\"):\n",
    "    rekognition = boto3.client(\"rekognition\", region)\n",
    "    response = rekognition.detect_faces(\n",
    "        Image={\n",
    "            \"S3Object\": {\n",
    "            \"Bucket\": bucket,\n",
    "            \"Name\": key,\n",
    "            }\n",
    "        },\n",
    "        Attributes=attributes,\n",
    "    )\n",
    "    return response['FaceDetails']\n",
    "\n",
    "\n",
    "KEY = KEYS[0]\n",
    "for face in detect_faces(BUCKET, KEY):\n",
    "    for emotion in face['Emotions']:\n",
    "        real_emotion = unique_emotions.index(KEY.split('/')[0])\n",
    "        predicted_emotion = related_emotion.index(emotion['Type'])\n",
    "        break\n",
    "\n",
    "print(\"Predicted Value: \" + unique_emotions[predicted_emotion] + \"\\nReal Value: \" + unique_emotions[real_emotion])\n",
    "PATH_NAME = \"../valid_processed/\" + unique_emotions[real_emotion] + \"/0\" + KEY.split(\"/\")[1]\n",
    "plt.imshow(cv2.imread(PATH_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building a CNN using Keras and Tensorflow </h2>\n",
    "\n",
    "Using the Tensorflow and Keras libraries, we made a CNN model that would accurately predict incoming testing images fed into the model. This CNN model consisted of a combination of convolutional (with 38-42 filters in each), max pooling, and dense layers; our main activation function was ReLU due to its faster computational run time. The hyperparameters were tweaked accordingly to provide the best model. Some notable tweaks were to the batch size (16), the number of epochs (12), and the measure of loss (categorical cross-entropy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31515 images belonging to 8 classes.\n",
      "Found 21850 images belonging to 8 classes.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
    "\n",
    "# Loading in training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../train_processed',\n",
    "        target_size=(50, 50),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Loading in testing data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "        '../valid_processed',\n",
    "        target_size=(50, 50),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Initializing sequential model that makes use of convolutional, pooling, and dense layers\n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu', input_shape=(50, 50, 3), padding='same'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same'))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=48, kernel_size=3, activation='relu', padding = 'same'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding = 'same'))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding = 'same'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same'))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1969/1970 [============================>.] - ETA: 0s - loss: 1.2270 - acc: 0.5026Epoch 1/12\n",
      "1970/1970 [==============================] - 334s 170ms/step - loss: 1.2269 - acc: 0.5026 - val_loss: 1.4578 - val_acc: 0.5033\n",
      "Epoch 2/12\n",
      " 636/1970 [========>.....................] - ETA: 2:53 - loss: 0.8738 - acc: 0.6559"
     ]
    }
   ],
   "source": [
    "# NOTE: Do not run this - Just for showing the code\n",
    "\n",
    "# Finally compile and train the cnn\n",
    "cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "cnn.fit(x=train_generator, validation_data=test_generator, epochs=12)\n",
    "\n",
    "cnn.save('model/test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Testing the Built Model </h1>\n",
    "\n",
    "First, the trained model was loaded into the Sagemaker notebook instance to check its performance over a variety of testing images. Note that these testing images were resized from 350x350 to 50x50 since the model was trained on 50x50 images. In future cases, these images can be resized to their original shape; however, for the purposes of time, we trained it on a smaller image size.\n",
    "\n",
    "Since these testing images do not have any labels assigned to them, the model could only accurately be checked through use of visual inspection. In order to provide the most optimal method of doing this, we first added up the counts of the model's prediction of the testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we appended the file paths to each subsequent emotion depending on what the model predicted the testing image to be. With this, we can see the image along with the subsequent emotion that the model predicted. For example, if we pull out a particular image from the array (like the the sadness one) and see if the image emulates that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(filepath):\n",
    "    IMG_SIZE = 50  # 50 in txt-based\n",
    "    img_array = cv2.imread(filepath)  # read in the image, convert to grayscale\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize image to match model's expected sizing\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 3)  # return the image with shaping that TF wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "unique_emotions = ['anger', 'surprise', 'disgust', 'fear', 'neutral', 'happiness', 'sadness', 'contempt']\n",
    "new_model = tf.keras.models.load_model('../model/my_model')\n",
    "\n",
    "unique_pictures = {'anger' : [],\n",
    " 'surprise' : [],\n",
    " 'disgust' : [],\n",
    " 'fear' : [],\n",
    " 'neutral' : [],\n",
    " 'happiness' : [],\n",
    " 'sadness' : [],\n",
    " 'contempt' : []}\n",
    "\n",
    "for test_file in os.listdir(\"../test_resize\"):\n",
    "    #print(test_file)\n",
    "    prediction = new_model.predict([prepare(\"../test_resize/\" + test_file)])\n",
    "    prediction_list = prediction.tolist()\n",
    "    prediction_list = prediction_list[0]\n",
    "    max_value = max(prediction_list)\n",
    "    maximum = -1\n",
    "    for index in prediction_list:\n",
    "        if (maximum < index):\n",
    "            maximum = index\n",
    "    unique_emotion = unique_emotions[prediction_list.index(maximum)]\n",
    "    \n",
    "    unique_pictures[unique_emotion].append(\"../test_resize/\" + test_file)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print('Example of anger predicted from image using model:')\n",
    "plt.imshow(cv2.imread(unique_pictures['anger'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Challenges </h2>\n",
    "\n",
    "<h3> Training Using Tensorflow and AWS Sagemaker </h3>\n",
    "\n",
    "During this project, a repository of images were used for training the convolutional neural network from a preexisting dataset. Although this dataset allowed us to train the CNN, it provided problems in terms of training time and accuracy of classification. Due to the size of the images, we converted the images from 350x350 to 50x50, so that our model would train faster (as Sagemaker automatically logs itself out around every 12 hours, so our effective training time is reduced to that amount) considering cost and time constraints. Reducing the size of the images significantly lowered our training and validation accuracy; however, we hope that later, we will be able to use a larger instance for the complete training of the model.\n",
    "\n",
    "<h3> Accuracy of the Dataset </h3>\n",
    "\n",
    "The accuracy of the classification was 56%, while our training accuracy was 98%. After numerous changes to hyperparameters (including early stopping, learning rate, etc.) and the model itself, we weren't able to increase the validation accuracy and its erratic behavior while training (spiked up and down for each epoch). Running the data on one of the best widely-known image classification models (Amazon Rekognition), we saw that it had an accuracy of 60%, indicating that this dataset had inconsistent classifications or low-quality images. However, unfortunately, due to our limited access of datasets, we were unable to procure a better one at this time. We believe our model will perform better with a more accurate dataset, and will scale linearly with the accuracy of Rekognition's image classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
